{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Model Optimization Toolkit\n",
    "Is a suite of techniques that allows to optimize machine learning models for deployment and execution. The notebook shows the difference between simple tensorflow models and the same models after applying the __Model Optimization Toolkit(MOT)__. More about this toolkit can find [here](https://www.tensorflow.org/performance/model_optimization) and [here](https://www.tensorflow.org/performance/post_training_quantization). Two types of models will be used:\n",
    "* __VGG16__\n",
    "* __ResNet34__\n",
    "\n",
    "These models have totally dissimilar architecture, the question is how Model Optimization Toolkit cope with them. These models will be train on two datasets:\n",
    "* __CIFAR10__\n",
    "* __CIFAR100__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "import tensorflow as tf\n",
    "from copy import deepcopy\n",
    "\n",
    "from dataset.opensets import CIFAR10, CIFAR100\n",
    "from dataset.models.tf import VGG16, ResNet34\n",
    "from dataset import Pipeline, B, V, C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training and testing model you need to create pipelines, to avoid a large amount of repetitive code, combine the creation of the pipeline into a separate function named __create_pipelines.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipelines(model, data, name, config):\n",
    "    \"\"\"Created pipelines with given parameters.\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : Dataset model\n",
    "        model for training and testing\n",
    "    data : Dataset\n",
    "        the data on which the model will be train\n",
    "    name : str\n",
    "        the name of pipelines\n",
    "    config : dict\n",
    "        configuration dict to current model\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    train_pipeline : Dataset pipeline\n",
    "        pipeline for training current model\n",
    "    test_pipeline : Dataset pipeline\n",
    "        pipeline for testing current model\n",
    "    \"\"\"\n",
    "    train_pipeline = ((\n",
    "        Pipeline()\n",
    "        .init_variable('loss', init_on_each_run=list)\n",
    "        .init_variable('loss_history', init_on_each_run=list)  \n",
    "        .init_model('dynamic', model, name, config=config)\n",
    "        .to_array()\n",
    "        .train_model(name, fetches='loss', \n",
    "                     feed_dict={'images': B('images'), 'labels': B('labels')},\n",
    "                     save_to=V('loss'))\n",
    "        .update_variable('loss_history', V('loss'), mode='a')\n",
    "    ) << data.train)\n",
    "\n",
    "\n",
    "    test_pipeline = ((\n",
    "        Pipeline()\n",
    "        .import_model(name, train_pipeline)\n",
    "        .init_variable('predictions', init_on_each_run=list)\n",
    "        .init_variable('metrics', init_on_each_run=None)\n",
    "        .to_array()\n",
    "        .predict_model(name, fetches='predictions', \n",
    "                       feed_dict={'images': B('images'), 'labels': B('labels')},\n",
    "                       save_to=V('predictions'))\n",
    "        .gather_metrics('class', targets=B('labels'), predictions=V('predictions'),\n",
    "                        fmt='logits', axis=-1, save_to=V('metrics'), mode='w')\n",
    "    ) << data.test)\n",
    "    return train_pipeline, test_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR 10\n",
    "### Define a dataset\n",
    "Firstly create a dataset instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting...\n",
      "Extracted\n",
      "Downloading cifar-100-python.tar.gz ...\n",
      "Downloaded cifar-100-python.tar.gz\n",
      "Extracting...\n",
      "Extracted\n"
     ]
    }
   ],
   "source": [
    "cifar10 = CIFAR10()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configurate the parameters for certain models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_config = {\n",
    "    'inputs': dict(images=dict(shape=B('image_shape')),\n",
    "                   labels=dict(classes=10, transform='ohe', name='targets')),\n",
    "    'input_block/inputs': 'images',\n",
    "    'loss': 'ce',\n",
    "    'optimizer': 'Adam',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By function described above, create pipelines for each models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_train_10_ppl, vgg_test_10_ppl = create_pipelines(VGG16, cifar10, 'cifar10_vgg16', cifar10_config)\n",
    "resnet_train_10_ppl, resnet_test_10_ppl = create_pipelines(ResNet34, cifar10,'cifar10_resnet34', cifar10_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can train models by running the pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_train_ppl.run(50, shuffle=True, n_epochs=3, drop_last=True, bar=True)\n",
    "resnet_train_ppl.run(50, shuffle=True, n_epochs=3, drop_last=True, bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models Optimization Toolkit\n",
    "Now let's apply the MOT to each trained on cifar10 models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter=tf.contrib.lite.TocoConverter.from_saved_model(saved_model_dir)\n",
    "converter.post_training_quantize=True\n",
    "tflite_quantized_model = converter.convert()\n",
    "# open(\"quantized_model.tflite\", \"wb\").write(tflite_quantized_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR 100\n",
    "### Define a dataset\n",
    "Firstly create a dataset instance, in this turn is cifar100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar100 = CIFAR100()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models creation\n",
    "As same as with cifar10, you have to create a configuration dictionary with current parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar100_config = {\n",
    "    'inputs': dict(images=dict(shape=B('image_shape')),\n",
    "                   labels=dict(classes=100, transform='ohe', name='targets')),\n",
    "    'input_block/inputs': 'images',\n",
    "    'loss': 'ce',\n",
    "    'optimizer': 'Adam',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function __create_pipelines__ will be useful again, it helps you to create a new instance of pipelines with the cifar100 inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_train_100_ppl, vgg_test_100_ppl = create_pipelines(VGG16, cifar100, 'cifar100_vgg16', cifar100_config)\n",
    "resnet_train_100_ppl, resnet_test_100_ppl = create_pipelines(ResNet34, cifar100,'cifar100_resnet34', cifar100_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training models\n",
    "After all preparations you are able to train your models, let's do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_train_ppl.run(50, shuffle=True, n_epochs=3, drop_last=True, bar=True)\n",
    "resnet_train_ppl.run(50, shuffle=True, n_epochs=3, drop_last=True, bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models Optimization Toolkit\n",
    "Now let's apply the MOT to each trained on cifar100 models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter=tf.contrib.lite.TocoConverter.from_saved_model(saved_model_dir)\n",
    "converter.post_training_quantize=True\n",
    "tflite_quantized_model = converter.convert()\n",
    "# open(\"quantized_model.tflite\", \"wb\").write(tflite_quantized_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
